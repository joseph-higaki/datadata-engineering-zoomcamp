# Batch - SPark

## Install local Spark
 [Install Spark / PySpark](install_local_pyspark.md#install-spark-on-my-dev-container) at the dev containr

And testing if [it works](./testing_dev_container_spark.ipynb)

## Installed GCP VM Spark
Install Spark [at GCP CLoud](install_local_pyspark.md#install-spark-on-gcp-cloud)

Testing [here](./testing_gcp_vm_spark.ipynb)

## 01 Jupyter Notebook Spark 
Running a cloud VM: `de-zoomcamp-jhigaki-course` `spark-instance` on `europe-southwest1-a`
Installed:
* [Java ](./install_local_pyspark.md#install-java-1)  openjdk-11.0.2
* [anaconda](./install_local_pyspark.md#install-python-conda)  Anaconda3-2024.10-1-Linux-x86_64
* Python 3.11 
* [Spark](./install_local_pyspark.md#install-spark-1) 3.4.4

### [01 Taxi Rides Spark Notebook](./01_taxy_rides_spark.ipynb)
* Reading local parquet
* Queing Transformations executing tasks